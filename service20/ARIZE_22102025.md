# Arize AX Setup Guide - Agent Graph Visualization

**Date**: October 22, 2025
**Product**: Arize AX (EU Region)
**Dashboard**: https://app.eu-west-1a.arize.com/organizations/QWNjb3VudE9yZ2FuaXphdGlvbjoxNzQ6UmVJZw==/spaces/U3BhY2U6MjUzOlA5amY=

## Overview

This guide explains how to set up Arize AX tracing for Python services to enable LLM observability and agent workflow visualization. Traces will appear in the Arize AX dashboard with hierarchical agent graphs showing relationships between different agents and their tasks.

## Prerequisites

### 1. Arize AX Account Credentials

You need the following from your Arize AX account:

- **Space ID**: `U3BhY2U6MjUzOlA5amY=`
- **Service API Key**: `ak-26600c96-a523-4dfe-9ae6-ec62af9cce70-X19zTaST57xJQ11QJhPWTOtjOtRu4kG_`
- **Region**: EU (eu-west-1a)
- **Organization ID**: `QWNjb3VudE9yZ2FuaXphdGlvbjoxNzQ6UmVJZw==`

### 2. Required Python Packages

Add to `pyproject.toml` or `requirements.txt`:

```toml
dependencies = [
    "arize-otel>=0.5.0",           # Arize OpenTelemetry integration
    "openinference-instrumentation-langchain>=0.1.0",  # LangChain auto-instrumentation
    "opentelemetry-sdk>=1.20.0",   # OpenTelemetry SDK
    "opentelemetry-api>=1.20.0",   # OpenTelemetry API
    "langchain-openai>=0.1.0",     # LangChain OpenAI integration
    "langchain-core>=0.1.0",       # LangChain core
]
```

Install with:
```bash
pip install arize-otel openinference-instrumentation-langchain opentelemetry-sdk opentelemetry-api langchain-openai langchain-core
```

## Setup Steps

### Step 1: Environment Configuration

Add to your `.env` file:

```bash
# Arize AX Configuration (EU Region)
# Organization: QWNjb3VudE9yZ2FuaXphdGlvbjoxNzQ6UmVJZw==
# Space: U3BhY2U6MjUzOlA5amY=
# Dashboard: https://app.eu-west-1a.arize.com/organizations/QWNjb3VudE9yZ2FuaXphdGlvbjoxNzQ6UmVJZw==/spaces/U3BhY2U6MjUzOlA5amY=
ARIZE_SPACE_KEY=U3BhY2U6MjUzOlA5amY=
ARIZE_API_KEY=ak-26600c96-a523-4dfe-9ae6-ec62af9cce70-X19zTaST57xJQ11QJhPWTOtjOtRu4kG_
```

### Step 2: Initialize Arize Tracing (Basic)

For simple LLM call tracing without agent graphs:

```python
import os
from dotenv import load_dotenv
from arize.otel import register
from arize.otel.otel import Transport, Endpoint
from openinference.instrumentation.langchain import LangChainInstrumentor

# Load environment variables
load_dotenv()

def setup_arize_tracing():
    """Setup Arize AX tracing for LLM calls."""

    # Get credentials from environment
    ARIZE_SPACE_KEY = os.getenv("ARIZE_SPACE_KEY")
    ARIZE_API_KEY = os.getenv("ARIZE_API_KEY")

    # Register with Arize AX
    tracer_provider = register(
        space_id=ARIZE_SPACE_KEY,
        api_key=ARIZE_API_KEY,
        project_name="your-service-name",  # Change this to your service name
        endpoint=Endpoint.ARIZE_EUROPE,    # Use built-in EU endpoint
        transport=Transport.HTTP,           # Required for EU region
    )

    # Instrument LangChain for automatic tracing
    LangChainInstrumentor().instrument(tracer_provider=tracer_provider)

    return tracer_provider

# Initialize tracing at the start of your application
tracer_provider = setup_arize_tracing()

# Now all LangChain LLM calls will be automatically traced
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
response = llm.invoke([HumanMessage(content="Your query here")])
```

### Step 3: Add Agent Graph Metadata (Advanced)

For agent workflow visualization with hierarchical graphs:

```python
import os
from dotenv import load_dotenv
from arize.otel import register
from arize.otel.otel import Transport, Endpoint
from openinference.instrumentation.langchain import LangChainInstrumentor
from opentelemetry import trace
from opentelemetry.trace import get_tracer

load_dotenv()

def setup_arize_tracing():
    """Setup Arize AX tracing with agent metadata support."""

    ARIZE_SPACE_KEY = os.getenv("ARIZE_SPACE_KEY")
    ARIZE_API_KEY = os.getenv("ARIZE_API_KEY")

    tracer_provider = register(
        space_id=ARIZE_SPACE_KEY,
        api_key=ARIZE_API_KEY,
        project_name="your-service-name",
        endpoint=Endpoint.ARIZE_EUROPE,
        transport=Transport.HTTP,
    )

    LangChainInstrumentor().instrument(tracer_provider=tracer_provider)

    return tracer_provider

# Initialize tracing
tracer_provider = setup_arize_tracing()

# Get tracer for manual span creation
tracer = get_tracer(__name__)

# Create agent workflow with graph metadata
def run_agent_workflow(query: str):
    """Run a multi-agent workflow with graph visualization."""

    # Top-level workflow span
    with tracer.start_as_current_span(
        name="Research Workflow",
        attributes={
            "graph.node.id": "research_workflow",          # Required: Unique node ID
            "workflow.type": "research",
            "workflow.version": "1.0.0",
        }
    ) as workflow_span:

        # Agent 1: Query Planner
        with tracer.start_as_current_span(
            name="Query Planner Agent",
            attributes={
                "graph.node.id": "query_planner",          # Required: Unique node ID
                "graph.node.parent_id": "research_workflow", # Required: Parent node ID
                "agent.role": "planner",
                "agent.task": "analyze_query",
            }
        ) as planner_span:
            # Your query planning logic here
            planner_span.set_attribute("query.input", query)
            planner_span.set_attribute("query.complexity", "medium")

        # Agent 2: Research Agent (with LLM call)
        with tracer.start_as_current_span(
            name="Research Agent",
            attributes={
                "graph.node.id": "research_agent",
                "graph.node.parent_id": "research_workflow",
                "agent.role": "researcher",
                "agent.task": "gather_information",
            }
        ) as research_span:
            # LLM call - automatically traced by LangChainInstrumentor
            from langchain_openai import ChatOpenAI
            from langchain_core.messages import HumanMessage

            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
            response = llm.invoke([HumanMessage(content=query)])

            research_span.set_attribute("research.model", "gpt-4o-mini")
            research_span.set_attribute("research.success", True)

        # Agent 3: Synthesis Agent
        with tracer.start_as_current_span(
            name="Synthesis Agent",
            attributes={
                "graph.node.id": "synthesis_agent",
                "graph.node.parent_id": "research_workflow",
                "agent.role": "synthesizer",
                "agent.task": "format_response",
            }
        ) as synthesis_span:
            # Your synthesis logic here
            synthesis_span.set_attribute("synthesis.format", "structured")

        # Set workflow completion attributes
        workflow_span.set_attribute("workflow.status", "completed")
        workflow_span.set_attribute("workflow.agents_used", 3)
        workflow_span.set_attribute("workflow.success", True)

    return response.content

# Run the workflow
result = run_agent_workflow("What are renewable energy opportunities in Copenhagen?")
```

## Key Concepts

### Required Attributes for Agent Graphs

1. **`graph.node.id`** (Required)
   - Unique identifier for each agent/node in the workflow
   - Must be unique within the workflow
   - Example: `"query_planner"`, `"research_agent"`, `"synthesis_agent"`

2. **`graph.node.parent_id`** (Required for child nodes)
   - ID of the parent node in the hierarchy
   - Omit for top-level/root nodes
   - Example: `"research_workflow"` (parent of child agents)

3. **Additional Metadata** (Optional but recommended)
   - `agent.role`: Role of the agent (e.g., "planner", "researcher", "synthesizer")
   - `agent.task`: Specific task being performed
   - `workflow.type`: Type of workflow (e.g., "research", "analysis")
   - Custom attributes relevant to your use case

### Span Hierarchy Example

```
Research Workflow (graph.node.id: "research_workflow")
├── Query Planner Agent (graph.node.id: "query_planner", parent_id: "research_workflow")
├── Research Agent (graph.node.id: "research_agent", parent_id: "research_workflow")
│   └── ChatOpenAI.chat (automatically traced by LangChain instrumentation)
└── Synthesis Agent (graph.node.id: "synthesis_agent", parent_id: "research_workflow")
```

## Configuration Details

### Endpoint Configuration

- **Region**: EU (eu-west-1a)
- **OTLP Endpoint**: `https://otlp.eu-west-1a.arize.com/v1`
- **Built-in Enum**: `Endpoint.ARIZE_EUROPE` (recommended)
- **Transport**: `Transport.HTTP` (required, NOT gRPC)

### Available Endpoints

The `arize.otel` package provides built-in endpoints:

```python
from arize.otel.otel import Endpoint

Endpoint.ARIZE          # US region (default)
Endpoint.ARIZE_EUROPE   # EU region (eu-west-1a)
```

### Available Transports

```python
from arize.otel.otel import Transport

Transport.HTTP   # Use this for cloud endpoints
Transport.GRPC   # For self-hosted deployments only
```

## Verifying Setup

### 1. Run Your Service

After implementing the tracing code, run your service:

```bash
python your_service.py
```

### 2. Check Dashboard

1. Go to: https://app.eu-west-1a.arize.com/organizations/QWNjb3VudE9yZ2FuaXphdGlvbjoxNzQ6UmVJZw==/spaces/U3BhY2U6MjUzOlA5amY=

2. Navigate to your project (e.g., "service20-agent-workflow")

3. Click on **Traces** tab to see individual LLM calls

4. Click on **Agents** tab to see the agent workflow graph

### 3. Expected Visualizations

**Traces View**:
- Individual LLM calls with input/output
- Token usage and latency metrics
- Span names from LangChain instrumentation

**Agents View** (with graph metadata):
- Visual graph showing agent workflow
- Hierarchical relationships between agents
- Performance metrics for each agent
- Input/output information at each step

### 4. Debugging

If traces don't appear:

1. **Check credentials**: Verify Space ID and API key are correct
2. **Wait 10-30 seconds**: Traces may take time to process
3. **Check console output**: Look for error messages during `register()`
4. **Verify endpoint**: Ensure using `Endpoint.ARIZE_EUROPE` for EU region
5. **Confirm transport**: Must use `Transport.HTTP` (not gRPC)

## Example Projects

### Test Scripts Included

1. **`test_arize_ax.py`**
   - Simple LLM call tracing
   - No agent graph metadata
   - Good for testing basic setup

2. **`test_arize_ax_with_agents.py`**
   - Full agent workflow with graph metadata
   - Demonstrates hierarchical agent relationships
   - Shows proper use of `graph.node.id` and `graph.node.parent_id`

Run tests:
```bash
cd "C:\Users\chriz\OneDrive\Documents\CNZ\UrbanZero2\UrbanZero\server_c\service20"
python test_arize_ax.py                    # Basic tracing
python test_arize_ax_with_agents.py        # Agent graphs
```

## Integration Patterns

### Pattern 1: CrewAI Integration

For CrewAI agents, wrap crew execution with spans:

```python
from crewai import Crew, Agent, Task

# Setup tracing
tracer_provider = setup_arize_tracing()
tracer = get_tracer(__name__)

# Create CrewAI agents
researcher = Agent(
    role="Researcher",
    goal="Research the topic",
    backstory="Expert researcher",
)

# Wrap crew execution with tracing
with tracer.start_as_current_span(
    name="CrewAI Research Workflow",
    attributes={
        "graph.node.id": "crewai_workflow",
        "workflow.type": "crewai",
    }
) as workflow_span:

    # Create tasks with individual spans
    with tracer.start_as_current_span(
        name="Research Task",
        attributes={
            "graph.node.id": "research_task",
            "graph.node.parent_id": "crewai_workflow",
            "agent.name": "Researcher",
        }
    ):
        crew = Crew(agents=[researcher], tasks=[...])
        result = crew.kickoff()

    workflow_span.set_attribute("workflow.status", "completed")
```

### Pattern 2: FastAPI Integration

For FastAPI services, initialize tracing on startup:

```python
from fastapi import FastAPI

app = FastAPI()

@app.on_event("startup")
async def startup_event():
    """Initialize Arize tracing on FastAPI startup."""
    setup_arize_tracing()
    print("Arize AX tracing initialized")

@app.post("/research")
async def research_endpoint(query: str):
    """Endpoint that uses traced agents."""
    result = run_agent_workflow(query)
    return {"result": result}
```

### Pattern 3: Async Operations

For async functions, tracing works the same way:

```python
async def async_agent_workflow(query: str):
    """Async agent workflow with tracing."""

    tracer = get_tracer(__name__)

    with tracer.start_as_current_span(
        name="Async Research Workflow",
        attributes={
            "graph.node.id": "async_workflow",
        }
    ) as workflow_span:

        # Async agent operations
        with tracer.start_as_current_span(
            name="Async Research Agent",
            attributes={
                "graph.node.id": "async_researcher",
                "graph.node.parent_id": "async_workflow",
            }
        ):
            result = await async_llm_call(query)

        workflow_span.set_attribute("workflow.status", "completed")

    return result
```

## Best Practices

### 1. Project Naming

Use descriptive project names that identify the service:
- ✅ `"service20-research-agent"`
- ✅ `"opportunity-matching-workflow"`
- ✅ `"city-analysis-pipeline"`
- ❌ `"test"` or `"my-project"`

### 2. Node ID Conventions

- Use lowercase with underscores: `"query_planner"`, `"research_agent"`
- Make IDs descriptive of the agent's role
- Keep consistent across similar workflows
- Don't use random/generated IDs (makes analysis harder)

### 3. Span Naming

- Use clear, human-readable names: `"Query Planner Agent"`, `"Research Agent"`
- Include agent type/role in the name
- Keep consistent capitalization

### 4. Metadata

Add relevant metadata to help with debugging:
- Input/output information
- Model names and parameters
- Success/failure status
- Error messages
- Performance metrics

### 5. Error Handling

Wrap tracing in try-catch to prevent failures:

```python
try:
    tracer_provider = setup_arize_tracing()
    print("Arize tracing enabled")
except Exception as e:
    print(f"Warning: Arize tracing failed: {e}")
    print("Continuing without tracing...")
    # Service continues to work even if tracing fails
```

## Troubleshooting

### Common Issues

#### 1. "Unable to validate authorization from span"

**Problem**: Wrong API key or Space ID
**Solution**: Verify credentials in `.env` file match Arize dashboard

#### 2. "404: Cannot POST /v1/traces"

**Problem**: Wrong endpoint
**Solution**: Use `Endpoint.ARIZE_EUROPE` (not custom URL)

#### 3. "StatusCode.UNIMPLEMENTED"

**Problem**: Using gRPC transport instead of HTTP
**Solution**: Explicitly set `transport=Transport.HTTP`

#### 4. Traces not appearing in dashboard

**Problem**: May take 10-30 seconds to process
**Solution**: Wait and refresh the dashboard

#### 5. Agent graph not showing

**Problem**: Missing `graph.node.id` or `graph.node.parent_id` attributes
**Solution**: Ensure both attributes are set on all child spans

## Additional Resources

- **Arize AX Documentation**: https://arize.com/docs/ax/
- **CrewAI Integration**: https://arize.com/docs/ax/integrations/python/crewai/crewai-tracing
- **Agent Metadata**: https://arize.com/docs/ax/observe/agents/implementing-agent-metadata-for-arize
- **OpenTelemetry**: https://opentelemetry.io/docs/

## Support

For issues or questions:
1. Check the Arize AX dashboard for error messages
2. Review console output for OpenTelemetry warnings
3. Verify all required packages are installed
4. Ensure environment variables are loaded correctly

---

**Last Updated**: October 22, 2025
**Tested With**: Service20 Open Deep Research
**Status**: ✅ Working - All traces successfully exported to Arize AX EU region
